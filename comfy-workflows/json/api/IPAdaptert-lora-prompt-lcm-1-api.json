{
  "3": {
    "inputs": {
      "seed": 944516346096519,
      "steps": 8,
      "cfg": 1,
      "sampler_name": "lcm",
      "scheduler": "sgm_uniform",
      "denoise": 1,
      "model": [
        "23",
        0
      ],
      "positive": [
        "10",
        2
      ],
      "negative": [
        "7",
        0
      ],
      "latent_image": [
        "5",
        0
      ]
    },
    "class_type": "KSampler"
  },
  "4": {
    "inputs": {
      "ckpt_name": "SD1.5/epicrealism_naturalSinRC1VAE.safetensors"
    },
    "class_type": "CheckpointLoaderSimple"
  },
  "5": {
    "inputs": {
      "width": 512,
      "height": 768,
      "batch_size": 1
    },
    "class_type": "EmptyLatentImage"
  },
  "7": {
    "inputs": {
      "text": "text, watermark",
      "clip": [
        "4",
        1
      ]
    },
    "class_type": "CLIPTextEncode"
  },
  "8": {
    "inputs": {
      "samples": [
        "3",
        0
      ],
      "vae": [
        "4",
        2
      ]
    },
    "class_type": "VAEDecode"
  },
  "10": {
    "inputs": {
      "wildcard_text": "<lora:SD1.5/cloth/2b leotardV2:1.0:1.0:LBW=0,0,0,0,0,0,0,A,A,B,B,0,0,0,0,0,0;A=2.0;B=1.5>, 2b leotard, (covered eyes:1.1), hair over eyes",
      "populated_text": "<lora:SD1.5/cloth/2b leotardV2:1.0:1.0:LBW=0,0,0,0,0,0,0,A,A,B,B,0,0,0,0,0,0;A=2.0;B=1.5>, 2b leotard, (covered eyes:1.1), hair over eyes",
      "mode": true,
      "Select to add LoRA": "Select the LoRA to add to the text",
      "Select to add Wildcard": "Select the Wildcard to add to the text",
      "seed": "fixed",
      "model": [
        "4",
        0
      ],
      "clip": [
        "4",
        1
      ]
    },
    "class_type": "ImpactWildcardEncode"
  },
  "16": {
    "inputs": {
      "output_path": "[time(%Y-%m-%d)]",
      "filename_prefix": "ComfyUI",
      "filename_delimiter": "_",
      "filename_number_padding": 2,
      "filename_number_start": "false",
      "extension": "jpg",
      "quality": 80,
      "lossless_webp": "false",
      "overwrite_mode": "false",
      "show_history": "false",
      "show_history_by_prefix": "true",
      "embed_workflow": "true",
      "show_previews": "true",
      "images": [
        "8",
        0
      ]
    },
    "class_type": "Image Save"
  },
  "17": {
    "inputs": {
      "ipadapter_file": null
    },
    "class_type": "IPAdapterModelLoader"
  },
  "18": {
    "inputs": {
      "weight": 0.7000000000000001,
      "noise": 0.2,
      "weight_type": "channel penalty",
      "ipadapter": [
        "17",
        0
      ],
      "clip_vision": [
        "19",
        0
      ],
      "image": [
        "20",
        0
      ],
      "model": [
        "10",
        0
      ]
    },
    "class_type": "IPAdapterApply"
  },
  "19": {
    "inputs": {
      "clip_name": null
    },
    "class_type": "CLIPVisionLoader"
  },
  "20": {
    "inputs": {
      "image_path": "./ComfyUI/input/example.png",
      "RGBA": "false",
      "filename_text_extension": "true"
    },
    "class_type": "Image Load"
  },
  "23": {
    "inputs": {
      "sampling": "lcm",
      "zsnr": false,
      "model": [
        "18",
        0
      ]
    },
    "class_type": "ModelSamplingDiscrete"
  }
}